{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST test dataset\n",
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import helper library\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset\n",
    "(trainx, trainy), (testx, testy) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seeing the shapes of imported dataset\n",
    "trainx.shape, trainy.shape, testx.shape, testy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_random_train():\n",
    "    random_number = np.random.randint(0, 60000)\n",
    "    sample_X = trainx[random_number]\n",
    "    sample_y = trainy[random_number]\n",
    "    return sample_X, sample_y, random_number\n",
    "\n",
    "random_X, random_y, _ = choose_random_train()\n",
    "plt.imshow(random_X)\n",
    "plt.xlabel(random_y)\n",
    "\n",
    "cursor = 0\n",
    "def batch_generator_1000():\n",
    "    global cursor\n",
    "    print(cursor)\n",
    "    x_batch = trainx[cursor:cursor+1000]\n",
    "    y_batch = trainy[cursor:cursor+1000]\n",
    "    cursor += 1000\n",
    "    cursor %= 60000\n",
    "    return x_batch, y_batch\n",
    "\n",
    "# aax, aay = batch_generator_1000()\n",
    "# aax.shape, aay.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the layers\n",
    "\n",
    "##### Layers\n",
    "- Each layers maintains following associated datatypes:\n",
    "    - Information types\n",
    "        - name: Name of the layer\n",
    "        - parameters_count: Number of parameters of the layer\n",
    "    - Useful types\n",
    "        - Depends on the type of layer:\n",
    "            - Eg: For weights layer, it's the weight matrix\n",
    "            - Eg: For bias layer, it's the bias vector\n",
    "            - Eg: For the activation layer, it is nothing\n",
    "        - They can be abstracted away for the user of the layer\n",
    "\n",
    "- Each layer should have the following functions\n",
    "    - Apply (input) -> output\n",
    "        - It applies the layer's operation to it's input to give an output\n",
    "    - Input gradient (output gradient, input value, output value) -> gradient values for input\n",
    "        - It calculates the gradient for the input given the gradient values for the output.\n",
    "        - It takes the output gradient values, it's input values and it's output values, if it saves the computations\n",
    "    - Parameter gradient (output gradient, input value, output value) -> gradient values for parameters\n",
    "        - It calculates the gradient for the parameters using the output gradients, input values and output values\n",
    "        - If the layer doesn't have any parameters, it has to output zero (because we will sum the gradients, and we need something to sum it)\n",
    "    - dummy gradient () -> a zero of shape of parameters\n",
    "        - a zero gradient value of shape of the parameters\n",
    "    - learn (parameter gradient, learning rate)\n",
    "        - It modifies the parameter given the gradient of the parameters and the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function\n",
    "def expand_dims_and_transpose(x):\n",
    "    return np.transpose(np.expand_dims(x, 0))\n",
    "def expand_dims(x):\n",
    "    return np.expand_dims(x, 0)\n",
    "\n",
    "# layers\n",
    "class Flattener:\n",
    "    def __init__(self, shape):\n",
    "        self.name = f\"Flattener                 : {shape} to {np.prod(shape)}\\n\"\n",
    "        self.param_count = 0\n",
    "\n",
    "        self.from_shape = shape\n",
    "    \n",
    "    def apply(self, input_value):\n",
    "        return np.ndarray.flatten(input_value)\n",
    "    \n",
    "    def input_gradient(self, output_gradient, input_value=None, output_value=None):\n",
    "        return np.ndarray.reshape(output_gradient, self.from_shape)\n",
    "    \n",
    "    def parameter_gradient(self, output_gradient=None, input_value=None, output_value=None):\n",
    "        return 0\n",
    "    \n",
    "    def dummy_gradient(self):\n",
    "        return 0\n",
    "    \n",
    "    def learn(self, parameter_gradient_value=None, learning_rate=None):\n",
    "        pass\n",
    "\n",
    "class Scaler:\n",
    "    def __init__(self, scale_factor):\n",
    "        self.name = f\"Scaler with scale factor {scale_factor}\"\n",
    "        self.param_count = 0\n",
    "\n",
    "        self.scale_factor = scale_factor\n",
    "    \n",
    "    def apply(self, input_value):\n",
    "        return self.scale_factor * input_value\n",
    "    \n",
    "    def input_gradient(self, output_gradient, input_value=None, output_value = None):\n",
    "        return self.scale_factor * output_gradient\n",
    "    \n",
    "    def parameter_gradient(self, output_gradient, input_value=None, output_value=None):\n",
    "        return 0\n",
    "    \n",
    "    def dummy_gradient(self):\n",
    "        return 0\n",
    "    \n",
    "    def learn(self, parameter_gradient_value=None, learning_rate=None):\n",
    "        pass\n",
    "\n",
    "class Weights:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.name = f\"Weights layer             : {input_size} to {output_size}\"\n",
    "        self.param_count = input_size * output_size\n",
    "        \n",
    "        # We create a transposed weights because input is a row vector\n",
    "        self.weights = np.random.rand(input_size, output_size) - .5\n",
    "\n",
    "    def apply(self, input_value):\n",
    "        return np.matmul(input_value, self.weights)\n",
    "    \n",
    "    def input_gradient(self, output_gradient, input_value=None, output_value=None):\n",
    "        transposed_output_gradient = expand_dims_and_transpose(output_gradient)\n",
    "        return np.transpose(np.matmul(self.weights, transposed_output_gradient))[0,:]\n",
    "    \n",
    "    def parameter_gradient(self, output_gradient, input_value, output_value=None):\n",
    "        transposed_input = expand_dims_and_transpose(input_value)\n",
    "        # print(np.shape(transposed_input), np.shape(output_gradient))\n",
    "        return np.matmul(transposed_input, expand_dims(output_gradient))\n",
    "    \n",
    "    def dummy_gradient(self):\n",
    "        return np.zeros(np.shape(self.weights))\n",
    "    \n",
    "    def learn(self, parameter_gradient_value, learning_rate):\n",
    "        self.weights -= learning_rate * parameter_gradient_value\n",
    "\n",
    "class Biases:\n",
    "    def __init__(self, input_size):\n",
    "        self.name = f\"Biases layer              : {input_size}\"\n",
    "        self.param_count = input_size\n",
    "\n",
    "        # We create a row vector of biases because input is a row vector\n",
    "        self.biases = np.random.rand(input_size) - .5\n",
    "    \n",
    "    def apply(self, input_value):\n",
    "        return input_value + self.biases\n",
    "    \n",
    "    def input_gradient(self, output_gradient, input_value=None, output_value=None):\n",
    "        return output_gradient\n",
    "    \n",
    "    def parameter_gradient(self, output_gradient, input_value=None, output_value=None):\n",
    "        return output_gradient\n",
    "    \n",
    "    def dummy_gradient(self):\n",
    "        return np.zeros(np.shape(self.biases))\n",
    "    \n",
    "    def learn(self, parameter_gradient_value, learning_rate):\n",
    "        self.biases -= learning_rate * parameter_gradient_value\n",
    "\n",
    "class ReluActivation:\n",
    "    def __init__(self):\n",
    "        self.name = f\"ReLU Activation\\n\"\n",
    "        self.param_count = 0\n",
    "\n",
    "        # we create a vectorized function for activaton\n",
    "        self.activation_function = np.vectorize(lambda x: 0 if x < 0 else x)\n",
    "        # similarly we create a vectorized function for gradient\n",
    "        self.gradient_function = np.vectorize(lambda x: 0 if x < 0 else 1)\n",
    "    \n",
    "    def apply(self, input_value):\n",
    "        return self.activation_function(input_value)\n",
    "    \n",
    "    def input_gradient(self, output_gradient, input_value, output_value=None):\n",
    "        return np.multiply(output_gradient, self.gradient_function(input_value))\n",
    "    \n",
    "    def parameter_gradient(self, output_gradient=None, input_value=None, output_value=None):\n",
    "        return 0\n",
    "    \n",
    "    def dummy_gradient(self):\n",
    "        return 0\n",
    "    \n",
    "    def learn(self, parameter_gradient_value=None, learning_rate=None):\n",
    "        pass\n",
    "\n",
    "class SoftmaxActivationOnlyWhenNeeded:\n",
    "    def __init__(self, need=False):\n",
    "        self.name = f\"Softmax Activation, Activated:{need}\\n\"\n",
    "        self.param_count = 0\n",
    "        self.need = need\n",
    "    \n",
    "    def actual_apply(self, input_value):\n",
    "        maximum = np.max(input_value)\n",
    "        new_data = np.exp(input_value-maximum)\n",
    "        denominator = np.sum(new_data)\n",
    "        return new_data/denominator\n",
    "        \n",
    "    \n",
    "    def apply(self, input_value):\n",
    "        if self.need:\n",
    "            return self.actual_apply(input_value)\n",
    "        return input_value\n",
    "    \n",
    "    def actual_input_gradient(self, output_gradient, input_value, output_value=None):\n",
    "        size = len(output_value)\n",
    "        if output_value == None:\n",
    "            output_value = self.actual_apply(input_value)\n",
    "        magic_matrix = np.diag(output_value) - np.matmul(np.transpose(output_value), output_value)\n",
    "        return np.matmul(output_gradient, magic_matrix)\n",
    "    \n",
    "    def input_gradient(self, output_gradient, input_value=None, output_value=None):\n",
    "        if self.need:\n",
    "            return self.actual_input_gradient(output_gradient, input_value, output_value)\n",
    "        return output_gradient\n",
    "    \n",
    "    def parameter_gradient(self, output_gradient=None, input_value=None, output_value=None):\n",
    "        return 0\n",
    "    \n",
    "    def dummy_gradient(self):\n",
    "        return 0\n",
    "    \n",
    "    def learn(self, parameter_gradient_value=None, learning_rate=None):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset transformation\n",
    "class OneHotEncoder:\n",
    "    def __init__(self, input_size):\n",
    "        self.name = \"One hot encoder of size : {input_size}\"\n",
    "        self.size = input_size\n",
    "    \n",
    "    def __call__(self, number):\n",
    "        output = np.zeros(self.size)\n",
    "        output[number] = 1\n",
    "        return output\n",
    "\n",
    "class Identity:\n",
    "    def __init__(self):\n",
    "        self.name = \"None\"\n",
    "    def __call__(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the Loss Function\n",
    "##### Loss function\n",
    "- Each loss function has following data types\n",
    "    - Name: Name of the loss function.\n",
    "\n",
    "- Each loss function should implement the following methods\n",
    "    - Apply(prediction, ground_truth) -> loss value\n",
    "        - It calculates the required loss function.\n",
    "    - Call(prediction, gournd_truth) -> loss value\n",
    "        - It calls the apply function under the hood.\n",
    "    - Prediction gradient(prediction, ground_truth) -> gradient values for output\n",
    "        - It returns the gradient value for the loss wrt prediction value.\n",
    "        - To calculate it, it is supplied with the prediction value and the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_function(input_value):\n",
    "    maximum = np.max(input_value)\n",
    "    new_data = np.exp(input_value-maximum)\n",
    "    denominator = np.sum(new_data)\n",
    "    return new_data/denominator\n",
    "\n",
    "class CrossEntropyLossFunctionFromLogits:\n",
    "    def __init__(self):\n",
    "        self.name = \"Cross Entropy Loss Function from the logits\"\n",
    "        self.loss_function = softmax_function\n",
    "    \n",
    "    def apply(self, prediction, ground_truth):\n",
    "        # ground_truth_class = np.argmax(ground_truth)\n",
    "        # return -np.log(softmax_function(prediction)[ground_truth_class])\n",
    "\n",
    "        softmax_output = softmax_function(prediction)\n",
    "        return np.sum(np.multiply(-ground_truth, np.log(softmax_output)))\n",
    "    \n",
    "    def __call__(self, prediction, ground_truth):\n",
    "        return self.apply(prediction, ground_truth)\n",
    "    \n",
    "    def prediction_gradient(self, prediction, ground_truth):\n",
    "        return np.sum(ground_truth)*softmax_function(prediction) - ground_truth\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the model\n",
    "##### Sequential model\n",
    "- Each sequential model should have following data types\n",
    "    - Information types\n",
    "        - Name: Name of the model\n",
    "    - Useful types\n",
    "        - Layers: A list of the sequential layers\n",
    "        - Loss function : The function that defines the loss\n",
    "        - Dataset transformation : The transformation that is applied to the output of the dataset\n",
    "\n",
    "        - Cumulative parameter gradients for all layers\n",
    "        - \n",
    "- Each sequential model should implement the following methods\n",
    "    - Informational / debug\n",
    "        - representation\n",
    "    - Useful\n",
    "        - inference (input) -> output\n",
    "            - For simple inference\n",
    "        - Reset gradient\n",
    "            - To reset the parameter gradients for all layers\n",
    "        - forward propagation (input) -> output\n",
    "            - For simple layer by layer forward propagation\n",
    "            - Saves the input to each layer\n",
    "            - Saves the last output\n",
    "            - Prepares for the back propagation\n",
    "        - Back propagation (output gradients) ->\n",
    "            - Layer by layer back propagation calculating the parameter gradients at each step, and back-propagating the intermediate gradients\n",
    "            - Sums the parameter gradients\n",
    "        - input gradients\n",
    "            - To calculate the gradient of the input given the gradient of the output\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential_Model:\n",
    "    def __init__(self, layers, loss_function, name=\"Sequential Model\",  dataset_transformation=Identity()):\n",
    "        self.name = name\n",
    "\n",
    "        self.layers = layers\n",
    "        self.loss_function = loss_function\n",
    "        self.dataset_transformation = dataset_transformation\n",
    "        self.param_count = sum(layer.param_count for layer in layers)\n",
    "        self.last_loss = 0\n",
    "    \n",
    "    def inference(self, input_value):\n",
    "        last_output = input_value\n",
    "        for layer in self.layers:\n",
    "            last_output = layer.apply(last_output)\n",
    "        return last_output\n",
    "\n",
    "    def __call__(self, input_value):\n",
    "        return self.inference(input_value)\n",
    "\n",
    "    def reset_gradient(self):\n",
    "        self.parameter_gradients = [layer.dummy_gradient() for layer in self.layers]\n",
    "        self.last_loss = 0\n",
    "    \n",
    "    def forward_propagation(self, input_value):\n",
    "        self.last_inputs = []\n",
    "\n",
    "        last_input = input_value\n",
    "        self.last_inputs.append(last_input)\n",
    "        for layer in self.layers:\n",
    "            last_input = layer.apply(last_input)\n",
    "            self.last_inputs.append(last_input)\n",
    "        return last_input\n",
    "    \n",
    "    def back_propagation(self, output_gradient):\n",
    "        last_gradient = output_gradient\n",
    "        last = len(self.layers) - 1\n",
    "        for j, layer in enumerate(self.layers[::-1]):\n",
    "            i = last - j\n",
    "            input_value = self.last_inputs[i]\n",
    "            output_value = self.last_inputs[i+1]\n",
    "            parameter_gradient = layer.parameter_gradient(last_gradient, input_value, output_value)\n",
    "\n",
    "            # save cumulative gradients\n",
    "            self.parameter_gradients[i] += parameter_gradient\n",
    "\n",
    "            last_gradient = layer.input_gradient(last_gradient, input_value, output_value)\n",
    "        return last_gradient\n",
    "    \n",
    "    def input_gradients(self, output_gradient):\n",
    "        last = len(self.layers()) - 1\n",
    "        for j, layer in enumerate(self.layers[::-1]):\n",
    "            i = last - j\n",
    "            output_gradient = layer.input_gradient(output_gradient, self.last_inputs[i], self.last_inputs[i+1])\n",
    "        return output_gradient\n",
    "    \n",
    "    def see_dataset_for_training(self, input_dataset, output_dataset):\n",
    "        ground_truth = self.dataset_transformation(output_dataset)\n",
    "        prediction = self.forward_propagation(input_dataset)\n",
    "        prediction_gradient = self.loss_function.prediction_gradient(prediction, ground_truth)\n",
    "        last_gradient = self.back_propagation(prediction_gradient)\n",
    "        self.last_loss += self.loss_function(prediction, ground_truth)\n",
    "        return last_gradient\n",
    "\n",
    "    def make_model_learn(self, learning_rate):\n",
    "        last = len(self.layers) - 1\n",
    "        for j, layer in enumerate(self.layers[::-1]):\n",
    "            i = last - j\n",
    "            layer.learn(self.parameter_gradients[i], learning_rate)\n",
    "        pass\n",
    "\n",
    "    def get_loss(self):\n",
    "        return self.last_loss\n",
    "    \n",
    "    def __repr__(self):\n",
    "        representation = f\"{self.name}\\n\\n\" \n",
    "        i = 0\n",
    "        for layer in self.layers:\n",
    "            i += 1\n",
    "            representation += f\"{i}. {layer.name}\\n\"\n",
    "        representation += f\"Total parameters = {self.param_count}\\n\"\n",
    "        representation += f\"Loss function = {self.loss_function.name}\\n\"\n",
    "        return representation\n",
    "        \n",
    "\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the model\n",
    "\n",
    "MnistClassificationModel = Sequential_Model([\n",
    "        Scaler(scale_factor=(1/255)),\n",
    "        Flattener(shape=(28,28)),\n",
    "    \n",
    "        Weights(28*28, 400),\n",
    "        Biases(400),\n",
    "        ReluActivation(),\n",
    "    \n",
    "        Weights(400, 200),\n",
    "        Biases(200),\n",
    "        ReluActivation(),\n",
    "\n",
    "        Weights(200, 100),\n",
    "        Biases(100),\n",
    "        ReluActivation(),\n",
    "    \n",
    "        Weights(100, 10),\n",
    "        Biases(10),\n",
    "    \n",
    "        SoftmaxActivationOnlyWhenNeeded(),\n",
    "    ],\n",
    "    loss_function=CrossEntropyLossFunctionFromLogits(),\n",
    "    name=\"Simple MNIST handwritten digit dataset classifier\",\n",
    "    dataset_transformation=OneHotEncoder(10)\n",
    ")\n",
    "# Reset cursor for training dataset if model is reinstiated\n",
    "cursor = 0\n",
    "losses = []\n",
    "# also reset the loss, because when model is reinstated training is gone, and so are losses\n",
    "\n",
    "print(MnistClassificationModel)\n",
    "# output = MnistClassificationModel(random_X)\n",
    "# print(output)\n",
    "# print(softmax_function(output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_tester(print_outputs=False):\n",
    "    rx, ry, n = choose_random_train()\n",
    "    prediction = (MnistClassificationModel(rx))\n",
    "\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.imshow(rx)\n",
    "    plt.xlabel(f\"Ground Truth = {ry}\\npredicted = {np.argmax(prediction)}\")\n",
    "    plt.title(f\"{n}th training example\")\n",
    "\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.imshow(np.reshape(prediction, (-1,1)))\n",
    "    plt.yticks(np.arange(10))\n",
    "    plt.xticks([])\n",
    "    \n",
    "    plt.subplot(1,3,3)\n",
    "    plt.imshow(np.reshape(softmax_function(prediction), (-1, 1)))\n",
    "    plt.yticks(np.arange(10))\n",
    "    plt.xticks([])\n",
    "    plt.ylabel(\"Output after softmax\")\n",
    "\n",
    "    plt.show()\n",
    "    if print_outputs:\n",
    "        print(\"Logits output = \", prediction)\n",
    "        print(\"Softmax output = \", softmax_function(prediction))\n",
    "        print(\"Loss value for this example = \", MnistClassificationModel.loss_function(prediction, ry))\n",
    "\n",
    "print(MnistClassificationModel)\n",
    "model_tester(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = 10\n",
    "epochs = 40\n",
    "last_epoch = -1\n",
    "learning_rate = 0.01\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\n======= Epoch : {epoch+1} =======\")\n",
    "    print(f\"Initial Loss = {MnistClassificationModel.get_loss()}\")\n",
    "    MnistClassificationModel.reset_gradient()\n",
    "    batchx, batchy = batch_generator_1000()\n",
    "    for tx, ty in zip(batchx, batchy):\n",
    "        MnistClassificationModel.see_dataset_for_training(tx, ty)\n",
    "        \n",
    "    MnistClassificationModel.make_model_learn(learning_rate/1000)\n",
    "    print(f\"Loss after training = {MnistClassificationModel.get_loss()}\")\n",
    "    losses.append(MnistClassificationModel.get_loss())\n",
    "\n",
    "    log_epoch = np.floor(np.log(epoch + 1)/np.log(epochs)*examples)\n",
    "    if log_epoch != last_epoch:\n",
    "        model_tester(print_outputs=True)\n",
    "    last_epoch = log_epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.savefig(\"Loss-graph.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_random_test():\n",
    "    limit = len(testy)\n",
    "    random_number = np.random.randint(0, limit)\n",
    "    sample_X = testx[random_number]\n",
    "    sample_y = testy[random_number]\n",
    "    return sample_X, sample_y, random_number\n",
    "    \n",
    "def choose_random_and_test_model(log_output=True):\n",
    "    xx, yy, n = choose_random_test()\n",
    "    prediction = MnistClassificationModel(xx)\n",
    "    prediction_number = np.argmax(prediction)\n",
    "\n",
    "    correct = prediction_number == yy\n",
    "    colour = 'blue' if correct else 'red'\n",
    "    plt.imshow(xx, cmap=\"Blues\")\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.xlabel(f\"Ground Truth = {yy}\\nPrediction = {prediction_number}\", color=colour)\n",
    "    plt.title(f\"{n}th test dataset\\n\")\n",
    "    return np.reshape(prediction, (-1,1))\n",
    "    \n",
    "choose_random_and_test_model() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,10, i*2+1)\n",
    "    prediction = choose_random_and_test_model()\n",
    "\n",
    "    plt.subplot(5, 10, i*2+2)\n",
    "    plt.imshow(prediction, cmap='Blues')\n",
    "    plt.yticks(np.arange(10))\n",
    "    plt.xticks([])\n",
    "\n",
    "plt.savefig(\"Output.png\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
